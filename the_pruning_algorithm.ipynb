{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Pruning is a process that removes a subset from the original word-level emotional dictionary according to some criteria. Three strategies of pruning are used to refine the dictionary, each of which uses a different criterion to eliminate a desired degree of words.  \n",
    "As stated earlier, the emotional dictionary is generated from the real-world news corpus, thus there contains topical common words. For example, consider a news corpus reporting by Google news. Words like “Google”, “news”, “report”, and reporters’ names in Google news are frequently occur and merged with other words in the emotional dictionary. Those words convey little social emotions and may disturb the effect of utilizing the dictionary. We prune those topical common words by a simple algorithm as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: The emotional dictionary OD generated by Eq. (9), pruning strategy (i.e., maximum,\n",
    "average, minimum)\n",
    "Output: The refined emotional dictionary RD. zero word document frequency: dfw;\n",
    "for each document d in N training articles do\n",
    "    for all words w in d do\n",
    "        if w not exists in d before then\n",
    "            dfw +1; \n",
    "        end if\n",
    "    end for \n",
    "end for\n",
    "for each word w in OD do\n",
    "    get theta, the probabilities of E emotions conditioned on w; calculate the value  lambda/w of theta according to the pruning strategy; \n",
    "    if  lambda/w is greater than or equal to dfw/N then\n",
    "        add w into RD; \n",
    "    end if\n",
    "end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm consists of two processes. Firstly, it calculates the document fre- quency of each word in the emotional dictionary, which can be conducted in the preprocessing. Secondly, it compares the relative document frequency with threshold lw for each word w, to determine whether w is added into the refined dictionary. As a result, the time complexity is O(W), where W is the number of unique words. The threshold lw is calculated by the pruning strategy and probabilities of E emotions conditioned on w, which are from the original emotional dictionary OD. Assume that a word “report” occurs in 5 documents, the total number of training articles N is 10, the specified pruning strategy is “minimum”, and the probabilities of 8 emotions conditioned on “report” are 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.2. Then, the relative document frequency of “report” is 0.5, and the threshold λ is 0.1. The word “report” is thus considered to be a topical common word and pruned from OD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
